{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo 5: NLP y Visualizaciones\n",
    "\n",
    "### 1. Objetivos:\n",
    "    - Aprender qué visualizaciones podemos realizar de nuestros procesamientos de lenguaje natural\n",
    " \n",
    "---\n",
    "    \n",
    "### 2. Desarrollo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya bien sabes, las visualizaciones son una parte esencial de nuestros análisis. Vamos a aprender cómo realizar algunas visualizaciones de los análisis estadísticos que ya hemos realizado. Esto nos ayudará a presentar nuestros hallazgos de manera más efectiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../../Datasets/new_york_times_bestsellers-clean.json')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rank.numberInt'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_by_title = df.groupby('title')['description'].max()\n",
    "grouped_by_title = grouped_by_title.str.lower()\n",
    "grouped_by_title = grouped_by_title.str.strip()\n",
    "grouped_by_title = grouped_by_title.str.replace('[^\\w\\s]', '')\n",
    "grouped_by_title = grouped_by_title.str.replace('\\d', '')\n",
    "grouped_by_title = grouped_by_title.str.replace('\\\\n', '')\n",
    "grouped_by_title = grouped_by_title.dropna()\n",
    "\n",
    "tokenized = grouped_by_title.apply(nltk.word_tokenize)\n",
    "all_words = tokenized.sum()\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "all_words_except_stop_words = [word for word in all_words if word not in english_stop_words]\n",
    "\n",
    "freq_dist = nltk.FreqDist(all_words_except_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos visualizar la frecuencia de las palabras más comunes\n",
    "\n",
    "most_common_20 = np.array(list(map(lambda x: list(x), freq_dist.most_common(20))))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_20[:, 0],y=most_common_20[:, 1].astype('int'), ax=ax, palette='RdYlBu');\n",
    "ax.set_title('Frecuencia de las 20 palabras más comunes', pad=10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_50 = np.array(list(map(lambda x: list(x), freq_dist.most_common(50))))\n",
    "\n",
    "fig = plt.figure(figsize=(6, 10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_50[:, 1].astype('int'), y=most_common_50[:, 0], ax=ax, palette='RdYlBu', orient='h');\n",
    "ax.set_title('Frecuencia de las 50 palabras más comunes', pad=10)\n",
    "ax.set_xlabel('Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos visualizar la frecuencia de los bigramas más comunes\n",
    "\n",
    "text = nltk.Text(all_words)\n",
    "freq_dist_bigrams = nltk.FreqDist(list(nltk.bigrams(text)))\n",
    "\n",
    "freq_dist_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_20 = np.array(list(map(lambda x: list(x), freq_dist_bigrams.most_common(20))),dtype=object)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_20[:, 0], y=most_common_20[:, 1], ax=ax, palette='RdYlBu');\n",
    "ax.set_title('Frecuencia de las 20 bigramas más comunes', pad=10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probemos sin palabras vacías\n",
    "\n",
    "text = nltk.Text(all_words_except_stop_words)\n",
    "freq_dist_bigrams = nltk.FreqDist(list(nltk.bigrams(text)))\n",
    "\n",
    "freq_dist_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common_20 = np.array(list(map(lambda x: list(x), freq_dist_bigrams.most_common(20))),dtype=object)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_20[:, 0], y=most_common_20[:, 1], ax=ax, palette='RdYlBu');\n",
    "ax.set_title('Frecuencia de las 20 bigramas más comunes sin palabras vacías', pad=10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No estamos restringidos solamente a bigramas\n",
    "\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(all_words)\n",
    "freq_dist_trigrams = nltk.FreqDist(list(ngrams(text, 3)))\n",
    "\n",
    "most_common_20 = np.array(list(map(lambda x: list(x), freq_dist_trigrams.most_common(20))),dtype=object)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_20[:, 0], y=most_common_20[:, 1], ax=ax, palette='RdYlBu');\n",
    "ax.set_title('Frecuencia de las 20 trigramas más comunes con palabras vacías', pad=10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(all_words_except_stop_words)\n",
    "freq_dist_trigrams = nltk.FreqDist(list(ngrams(text, 3)))\n",
    "\n",
    "most_common_20 = np.array(list(map(lambda x: list(x), freq_dist_trigrams.most_common(20))),dtype=object)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "sns.barplot(x=most_common_20[:, 0], y=most_common_20[:, 1], ax=ax, palette='RdYlBu');\n",
    "ax.set_title('Frecuencia de las 20 trigramas más comunes sin palabras vacías', pad=10)\n",
    "ax.set_ylabel('Count')\n",
    "ax.tick_params(axis='x', rotation=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# También podemos generar histogramas para visualizar la frecuencia de longitudes de palabras y oraciones\n",
    "\n",
    "word_lengths = [len(w) for w in all_words_except_stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(word_lengths, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = grouped_by_title.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(sentence_lengths, kde=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = grouped_by_title.str.split(' ').str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(num_of_words, kde=False, norm_hist=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar, vamos a hacer una nube de palabras utilizando la librería `wordcloud`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(max_font_size=100, background_color=\"white\").generate(' '.join(all_words_except_stop_words))\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
